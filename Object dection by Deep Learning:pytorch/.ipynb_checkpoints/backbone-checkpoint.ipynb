{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;当前物体检测算法的第一步通常是\n",
    "<font color='blue'>\n",
    "    利用卷积神经网络处理输入图像，生成深层的特征图，然后再利用各种算法完成区域生成与损失计算</font>\n",
    "    ,这部分卷积神经网络是整个检测算法的“骨架”，也被称为Backbone。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "网络backbone:\n",
    "<font color='red'>\n",
    "    <ul>\n",
    "        <li>VGGNet:走向深度</li>\n",
    "        <li>Inception:纵横交错</li>\n",
    "        <li>ResNet:里程碑的残差结构</li>\n",
    "        <li>densenet:多重残差</li>\n",
    "        <li>FPN:特征金字塔</li>\n",
    "        <li>DetNet:为检测而生</li>\n",
    "    </ul></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>1.神经网络的基本组成</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "网络中的各种层：\n",
    "<ul>\n",
    "    <li>卷积层：普通卷积，空洞卷积</li>\n",
    "    <li>激活函数层：Sigmoid\\Softmax</li>\n",
    "    <li>池化层：最大值池化、平均值池化</li>\n",
    "    <li>Dropout层：防止过拟合</li>\n",
    "    <li>BN层：缓解梯度消失，模型稳定</li>\n",
    "    <li>全连接层：分类</li>\n",
    "    </ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>1.1 卷积层</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;在深度学习中使用的卷积运算通常是离散的，\n",
    "<font color='red'>\n",
    "    卷积的本质是用卷积核的参数来提取数据的特征，通过矩阵点乘运算与求和运算来得到结果。</font><br/>\n",
    "    &emsp;卷积核参数与对应位置像素逐位相乘后累加作为一次计算的结果，然后在特征图上进行滑动，即可得到所有的计算结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "weight:\ttorch.Size([1, 1, 3, 3]),bias:\ttorch.Size([1])\n",
      "input:\ttorch.Size([1, 1, 5, 5]),ouput:\ttorch.Size([1, 1, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "#使用Convd()搭建卷积层\n",
    "conv = nn.Conv2d(in_channels=1,out_channels=1,kernel_size=3,stride=1,padding=1,dilation=1,groups=1,bias=True)\n",
    "#查看卷积核信息\n",
    "print(conv)\n",
    "#通过.weight与.bias查看卷积核的权重与偏置\n",
    "print('weight:\\t{},bias:\\t{}'.format(conv.weight.shape,conv.bias.shape))\n",
    "#输入特征图，需要注意特征必须是4维，第一维作为batch数\n",
    "input = torch.ones(1,1,5,5)\n",
    "output = conv(input)\n",
    "#当前配置的卷积核可以使输入和输出的大小一致\n",
    "print('input:\\t{},ouput:\\t{}'.format(input.shape,output.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于torch.nn.Conv2d(),传入的参数含义如下：\n",
    "<ul>\n",
    "    <li>in_channels:输入特征图的通道数，RGB图像通道数为3，卷积中特征图通道数一般是2的整数次幂。</li>\n",
    "    <li>out_channels:输出特征图的通道数。</li>\n",
    "    <li>kernek_size:卷积核的尺寸，常见的有1 、 3 、 5 、 7</li>\n",
    "    <li>stride:步长：即卷积核在特征图上滑动的步长，如果大于1，则输出特征图的尺寸会小于输入特征图的尺寸。</li>\n",
    "    <li>padding:填充，常见的有零填充、边缘填充等，pytorch默认零填充。</li>\n",
    "    <li>dilation:空洞卷积，当大于1可以增大感受野的同时保持特征图的尺寸</li>\n",
    "    <li>groups:可实现组卷积，即在卷积操作时不是逐点卷积，而是将输入通道分为多个组，稀疏连接达到降低计算量的目的。</li>\n",
    "    <li>bias:偏置</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>1.2激活层函数</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "神经网络仅仅通过线性的卷积运算的堆叠无法形成复杂的表达空间，因此需要加入<b>非线性的映射（激活函数）</b>,\n",
    "可以逼近任意的非线性函数，以提升整个神经网络的表达能力。在物体检测任务中，常用的激活函数有Sigmoid 、ReLU 、Softmax函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>1.2.1 Sigmoid函数</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sigmoid函数又称Logistic函数，模拟了生物的神经元特性，函数表达式如下\n",
    "$$\\sigma(x)=\\frac{1}{1+exp(-x)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[1., 1.],\n",
      "          [1., 1.]]]])\n",
      "tensor([[[[0.7311, 0.7311],\n",
      "          [0.7311, 0.7311]]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "input = torch.ones(1,1,2,2)\n",
    "print(input)\n",
    "sigmoid = nn.Sigmoid()\n",
    "print(sigmoid(input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sigmid函数可以用来做二分类，但其计算量较大并且容易出现梯度消失现象。在Sigmoid函数两侧的特征导数接近于0，这将导致在梯度反传时损失的误差难以传递到前面的网络层。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>ReLU函数<br/>Leaky ReLU<br/>Softmax</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>1.3 池化层</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;在卷积网络中，通常会在卷积层中增加池化层，以降低特征图的参数量，提升计算速度，增加感受野，是一种降采样操作。<br/>\n",
    "&emsp;在物体检测中，常用的池化有<b>最大值池化</b>与<b>平均值池化</b>，池化层有两个主要的输入参数，即<b>核尺寸</b>与<b>步长</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0.6444, -0.3068,  0.5785,  1.2349],\n",
      "          [ 0.4262,  0.6083,  0.1327,  0.2089],\n",
      "          [ 0.0494, -0.7280,  1.0194,  0.5135],\n",
      "          [-0.4067,  1.0226,  1.4180, -0.0288]]]])\n",
      "最大值池化:tensor([[[[0.6444, 1.2349],\n",
      "          [1.0226, 1.4180]]]]) ,,平均值池化:tensor([[[[ 0.3430,  0.5387],\n",
      "          [-0.0157,  0.7305]]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "max_pooling = nn.MaxPool2d(2,stride=2)\n",
    "aver_pooling = nn.AvgPool2d(2,stride=2)\n",
    "input = torch.randn(1,1,4,4)\n",
    "print(input)\n",
    "print('最大值池化:{} ,,平均值池化:{}'.format(max_pooling(input),aver_pooling(input)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>1.4 Dropout层</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropout的基本思想：<br/>\n",
    "&emsp;在训练时，每个神经元以概率p保留，即以1-p的概率停止工作，每次前向传播保留下来的神经元都不同，这样可以使得模型不太依赖于某些局部特征，泛化性能更强。<br/>\n",
    "&emsp;在测试时，为了保证相同的输出期望值，每个参数还要乘以p。<br/>\n",
    "Dropout可以防止过拟合的原因：\n",
    "<ul>\n",
    "    <li>多模型的平均</li>\n",
    "    <li>减少神经元间的依赖</li>\n",
    "    <li>生物进化</li>\n",
    "   </ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "#python将元素置0来实现Dropout层，第一个参数为置0概率，第二个参数为是否原地操作。\n",
    "dropout = nn.Dropout(0.5,inplace=False)\n",
    "input = torch.randn(2,64,7,7)\n",
    "output = dropout(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>1.5 BN层</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "随着网络的加深，网络变得难以训练收敛与调参，原因在于：<br/>\n",
    "&emsp;<font color='blue'>浅层参数的微弱变化经过多层线性变换与激活函数后会被放大，改变了每一层的输入分布，造成深层的网络需要不断调整以适应这些分布变化，最终导致模型难以训练收敛。</font><br/>\n",
    "&emsp;由于网络中参数变化导致内部节点数据分布发生变化的现象被称作ICS。ICS现象容易使训练进入饱和区，减慢网络的收敛。<font color='red'>BN层从改变数据分布的角度避免了参数陷入饱和区。</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BN层首先对每一个batch的输入特征进行白化操作，即去均值方差过程。假设一个batch的输入数据为$x : {x_1,...,x_m}$,首先求该batch的均值与方差，\n",
    "$$\\mu_{B}\\leftarrow\\frac{1}{m}\\sum^{m}_{i=1}x_i$$\n",
    "$$\\sigma^{2}_{B}\\leftarrow\\frac{1}{m}\\sum^{m}_{i=1}(x_{i}-\\mu_{B})^{2}$$\n",
    "其中，m代表batch的大小，$\\mu_B$为均值，$\\sigma^{2}_{B}$为方差。<br/>\n",
    "求得均值和方差后，进行去均值方差操作，\n",
    "$$\\hat x_i\\leftarrow\\frac{x_i-\\mu_B}{\\sqrt{\\sigma^2_B+\\varepsilon}}$$\n",
    "白化操作可以使输入的特征具有相同的均值与方差，固定了每一层的输入分布，从而加速网络的收敛。但白化操作也限制了网络中数据的表达能力，浅层学到的参数信息会被白化操作屏蔽掉，因此，BN层在白化操作后又增加了一个线性变换操作，让数据尽可能的恢复本身的表达能力，\n",
    "$$y_i\\leftarrow\\gamma\\hat x_i+\\beta$$\n",
    "$\\gamma$与$\\beta$为新引进的学习参数，最终的输出为$y_i$<br/>\n",
    "BN层的优点主要有3点：\n",
    "<ul>\n",
    "    <li>缓解梯度消失，加速网络收敛。</li>\n",
    "    <li>简化调参，网络更加稳定。</li>\n",
    "    <li>防止过拟合。</li>\n",
    "  </ul>\n",
    "但也存在一定的弊端：\n",
    "<ul>\n",
    "    <li>BN层要求较大的batch，而物体检测任务占用内存较高，会限制batch的大小，这会限制BN层有效地发挥归一化功能</li>\n",
    "    <li>数据的batch大小在训练与测试时往往不一样。在训练时一般滑动来计算平均值与方差，在测试时直接拿训练集的平均值与方差来用，这会导致测试集依赖于训练集。</li>\n",
    "  </ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "torch.Size([4, 64, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "#使用BN层需要传入一个参数为num_features,即特征的通道数\n",
    "bn = nn.BatchNorm2d(64)\n",
    "#esp为公式中的$\\varepsilon$,momentum为均值方差的动量，affine为添加可学习参数\n",
    "print(bn)\n",
    "input = torch.randn(4,64,224,224)\n",
    "output = bn(input)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>1.6 全连接层<h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;全连接层一般连接到卷积网络输出的特征图后边，特点是每一个节点都与上下层的所有节点相连，输入与输出都被延展成一维向量。<br/>\n",
    "&emsp;在物体检测算法中，卷积神经网络的主要作用是从局部到整体地提取图像的特征，而全连接层则用来将卷积抽象出的特征图进一步映射到特定维度的标签空间，以求取损失或输出预测结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:torch.Size([4, 1024]),output:torch.Size([4, 4096])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "#第一维表示有4个样本\n",
    "input = torch.randn(4,1024)\n",
    "linear = nn.Linear(1024,4096)\n",
    "output = linear(input)\n",
    "print('input:{},output:{}'.format(input.shape,output.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;全连接层的致命缺点在于其<font color='red'>参数量的庞大</font>，大量的参数会导致网络模型应用部署困难，并且存在大量的参数冗余，也容易发生过拟合现象。我们可以使用<font color='red'>全局平均池化层（GAP）</font>来取代全连接层。<br/>\n",
    "总体上，使用GAP有如下3点好处：\n",
    "<ul>\n",
    "    <li>利用池化实现了降维，极大地减少了网络的参数量。</li>\n",
    "    <li>将特征提取与分类合二为一，一定程度上可以防止过拟合。</li>\n",
    "    <li>由于去除了全连接层，可以实现任意图像尺度的输入。</li>\n",
    "  </ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>1.7 感受野</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;感受野是指<font color='blue'>特征图上的某个点能看到的输入图像的区域，即特征图上的点是由输入图像中感受野大小区域的计算得到的。</font><br/>\n",
    "&emsp;卷积层和池化层都会影响感受野,而激活函数曾通常对于感受野没有影响。对于一般的卷积神经网路，感受野可由以下公式计算得出：\n",
    "    $$RF_{l+1} = RF_l+(k-1)*s_l$$\n",
    "    $$S_l = \\prod^l_{i=1}Stride_i$$\n",
    "$RF_l$表示第l层的感受野，k代表l+1层卷积核的大小，$S_l$代表前l层的步长之积。当前层的步长并不影响当前层的感受野。<br/>\n",
    "通常来讲，Anchor的大小应该与感受野相匹配。<br/>\n",
    "在卷积网络中，有时还需要计算特征图的大小，一般可以按照下式进行计算：\n",
    "$$n_{out} = \\frac{n_{in}+2p-k}{s}+1$$\n",
    "$n_{in}$与$n_{out}$分别为输入特征图与输出特征图的尺寸，p代表这一层的padding大小，k代表这一层的卷积核大小，s为步长。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>1.8 空洞卷积</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;常见的图像分割算法通常使用池化层来增大感受野，同时也缩小了特征图尺寸，然后再利用上采样还原图像尺寸。特征图缩小再放大的过程造成了精度上的损失，因此需要有一种操作可以在<font color='red'>增加感受野的同时保持特征图的尺寸不变</font>,在这种需求下，空洞卷积就诞生了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;空洞卷积，就是卷积核中间带有一些洞，跳过一些元素进行卷积。在代码实现时，空洞卷积有一个额外的超参数dilation rate，表示空洞数。<br/>\n",
    "&emsp;假设空洞卷积的卷积核大小为k，空洞数为d，则其等效卷积核大小k'计算公式如下：\n",
    "$$k' = k+(k-1)*(d-1)$$\n",
    "空洞卷积的优点：在不引入额外参数的前提下可以任意扩大感受野，同时保持特征图的分辨率不变。感受野的扩大可以检测大物体，而特征图分辨率不变使得物体定位更加精准。<br/>\n",
    "&emsp;<font color='green'>Pytorch：在卷积时传入dilation参数即可。</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "普通卷积：Conv2d(3, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "空洞卷积: Conv2d(3, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), dilation=(2, 2))\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "#定义普通卷积，默认dilation为1\n",
    "conv1 = nn.Conv2d(3,256,3,stride=1,padding=1,dilation=1)\n",
    "#定义dilation为2的卷积\n",
    "conv2 = nn.Conv2d(3,256,3,stride=1,padding=1,dilation=2)\n",
    "print('普通卷积：{}\\n空洞卷积: {}'.format(conv1,conv2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "空洞卷积的缺点：\n",
    "<ul>\n",
    "    <li>网格效应：由于空洞卷积是一种稀疏的采样方式，当多个空洞卷积叠加时，有些像素没有被利用到，会损失信息的连续性与相关性。</li>\n",
    "    <li>远距离的信息没有相关性：空洞卷积采取了稀疏的采样方式，导致远距离卷积得到的结果之间缺乏相关性。</li>\n",
    "    <li>不同尺度物体的关系：大的dilation rate对于大物体分割与检测有利，但是对于小物体则有弊无利。</li>\n",
    "  </ul>\n",
    "<font color='purple'>解决：HDC结构</font>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "深度学习",
   "language": "python",
   "name": "dlspace"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
